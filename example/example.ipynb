{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compound-lease",
   "metadata": {},
   "source": [
    "# Example of running PhaseLink"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-consultation",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/TomSHudson/PhaseLink/blob/master/example/example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "Note that you need to change the run instance to GPU if using colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "objective-genome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify if running in google colab:\n",
    "use_google_colab = False\n",
    "\n",
    "# Install/add neccessary paths if using colab:\n",
    "if use_google_colab:\n",
    "    !pip install obspy\n",
    "    # Install nvidia-apex:\n",
    "    !git clone https://github.com/NVIDIA/apex\n",
    "    !pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "immune-oxygen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import neccessary modules:\n",
    "import sys, os, shutil\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import numpy as np \n",
    "import torch\n",
    "import gc \n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from obspy.geodetics.base import gps2dist_azimuth\n",
    "# if not use_google_colab:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# And import PhaseLink:\n",
    "if use_google_colab:\n",
    "  shutil.rmtree('./PhaseLink', ignore_errors=True)\n",
    "  !git clone https://github.com/TomSHudson/PhaseLink.git\n",
    "  sys.path.append('./PhaseLink/')\n",
    "else:\n",
    "  sys.path.append('..')\n",
    "import phaselink_dataset\n",
    "import phaselink_train\n",
    "import phaselink_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "filled-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy over example files into pwd if using colab:\n",
    "if use_google_colab:\n",
    "  !cp PhaseLink/example/params.json .\n",
    "  !cp PhaseLink/example/station_list.txt .\n",
    "  !cp PhaseLink/example/tt.pg .\n",
    "  !cp PhaseLink/example/tt.sg ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-logan",
   "metadata": {},
   "source": [
    "## 0. Load in param file and key info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "undefined-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import param json file:\n",
    "params_fname = \"params.json\"\n",
    "with open(params_fname, \"r\") as f:\n",
    "    params = json.load(f)\n",
    "    \n",
    "# Get GPU info if using colab:\n",
    "if use_google_colab:\n",
    "     print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "     params['device'] = \"cuda:0\" # Use first GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-harassment",
   "metadata": {},
   "source": [
    "## 1. Create a synthetic training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "confident-roller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting up...\n",
      "Starting thread 0\n",
      "Starting thread 1\n",
      "Starting thread 2\n",
      "Starting thread 3\n",
      "Starting thread 4\n",
      "Starting thread 5\n",
      "Starting thread 6\n",
      "Starting thread 7\n",
      "Creating the following files for the PhaseLink synthetic training dataset:\n",
      "station_map.pkl\n",
      "sncl_map.pkl\n",
      "shimane_train_X.npy\n",
      "shimane_train_Y.npy\n",
      "P-phases (zeros): 11973180 ( 98.0604422604 %)\n",
      "S-phases (ones): 236820 ( 1.93955773956 %)\n",
      "Saved the synthetic training dataset.\n"
     ]
    }
   ],
   "source": [
    "# Set key parameters from param file:\n",
    "max_picks = params['n_max_picks']\n",
    "t_max = params['t_win']\n",
    "n_threads = params['n_threads']\n",
    "\n",
    "print(\"Starting up...\")\n",
    "# Setup grid:\n",
    "phase_idx = {'P': 0, 'S': 1}\n",
    "lat0, lon0 = phaselink_dataset.get_network_centroid(params)\n",
    "stlo, stla, phasemap, sncl_idx, stations, sncl_map = \\\n",
    "    phaselink_dataset.build_station_map(params, lat0, lon0, phase_idx)\n",
    "x_min = np.min(stlo)\n",
    "x_max = np.max(stlo)\n",
    "y_min = np.min(stla)\n",
    "y_max = np.max(stla)\n",
    "for key in sncl_map:\n",
    "    X0, Y0 = stations[key]\n",
    "    X0 = (X0 - x_min) / (x_max - x_min)\n",
    "    Y0 = (Y0 - y_min) / (y_max - y_min)\n",
    "    stations[key] = (X0, Y0)\n",
    "\n",
    "# Save station maps for detect mode\n",
    "pickle.dump(stations, open(params['station_map_file'], 'wb'))\n",
    "pickle.dump(sncl_map, open(params['sncl_map_file'], 'wb'))\n",
    "\n",
    "# # Pwaves\n",
    "# pTT = tt_interp(params['tt_table']['P'], params['datum'])\n",
    "# print('Read pTT')\n",
    "# print('(dep,dist) = (0,0), (10,0), (0,10), (10,0):')\n",
    "# print('             {:.3f}   {:.3f}   {:.3f}   {:.3f}'.format(\n",
    "# pTT.interp(0,0), pTT.interp(10,0),pTT.interp(0,10),\n",
    "# pTT.interp(10,10)))\n",
    "\n",
    "# #Swaves\n",
    "# sTT = tt_interp(params['tt_table']['S'], params['datum'])\n",
    "# print('Read sTT')\n",
    "# print('(dep,dist) = (0,0), (10,0), (0,10), (10,0):')\n",
    "# print('             {:.3f}   {:.3f}   {:.3f}   {:.3f}'.format(\n",
    "# sTT.interp(0,0), sTT.interp(10,0),sTT.interp(0,10),\n",
    "# sTT.interp(10,10)))\n",
    "\n",
    "# Get travel-time tables for P and S waves:\n",
    "pTT = phaselink_dataset.tt_interp(params['trav_time_p'], params['datum'])\n",
    "sTT = phaselink_dataset.tt_interp(params['trav_time_s'], params['datum'])\n",
    "\n",
    "# Generate synthetic training dataset for given param file:\n",
    "in_q = mp.Queue() ###1000000)\n",
    "out_q = mp.Queue() ###1000000)\n",
    "proc = mp.Process(target=phaselink_dataset.output_thread, args=(out_q, params))\n",
    "proc.start()\n",
    "procs = []\n",
    "for i in range(n_threads):\n",
    "    print(\"Starting thread %d\" % i)\n",
    "    p = mp.Process(target=phaselink_dataset.generate_phases, \\\n",
    "        args=(in_q, out_q, x_min, x_max, y_min, y_max, \\\n",
    "              sncl_idx, stla, stlo, phasemap, pTT, sTT, params))\n",
    "    p.start()\n",
    "    procs.append(p)\n",
    "\n",
    "for i in range(params['n_train_samp']):\n",
    "    in_q.put(i)\n",
    "\n",
    "for i in range(n_threads):\n",
    "    in_q.put(None)\n",
    "#for p in procs:\n",
    "#    p.join()\n",
    "#proc.join()\n",
    "\n",
    "print(\"Creating the following files for the PhaseLink synthetic training dataset:\")\n",
    "print(params['station_map_file'])\n",
    "print(params['sncl_map_file'])\n",
    "print(params['training_dset_X'])\n",
    "print(params['training_dset_Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-sunday",
   "metadata": {},
   "source": [
    "## 2. Train the model using the syntehetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "architectural-french",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset info:\n",
      "Shape of X: (8140, 1500, 5) Shape of Y (8140, 1500)\n",
      "Begin training process.\n",
      "Epoch 1, 10% \t train_loss: 8.1183e-01 train_acc: 98.24% took: 38.19s\n",
      "Epoch 1, 20% \t train_loss: 4.4219e-01 train_acc: 98.07% took: 35.93s\n",
      "Epoch 1, 31% \t train_loss: 1.7653e-01 train_acc: 98.08% took: 35.90s\n",
      "Epoch 1, 41% \t train_loss: 1.5708e-01 train_acc: 98.06% took: 37.58s\n",
      "Epoch 1, 51% \t train_loss: 1.7691e-01 train_acc: 98.07% took: 36.36s\n",
      "Epoch 1, 62% \t train_loss: 1.9457e-01 train_acc: 98.06% took: 35.61s\n",
      "Epoch 1, 72% \t train_loss: 1.6042e-01 train_acc: 98.08% took: 35.39s\n",
      "Epoch 1, 82% \t train_loss: 1.6453e-01 train_acc: 98.07% took: 35.34s\n",
      "Epoch 1, 93% \t train_loss: 1.4670e-01 train_acc: 98.06% took: 36.86s\n",
      "Precision (Class 0): 0.980%\n",
      "Recall (Class 0): 1.000%\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-fdf18133d260>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m     model_path='./phaselink_model/')\n\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Begin training process.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_amp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable_amp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/python/github_repositories/PhaseLink_fork/PhaseLink/phaselink_train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, val_loader, n_epochs, enable_amp)\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precision (Class 0): {:4.3f}%\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprec_0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mprec_n_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Recall (Class 0): {:4.3f}%\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreca_0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mreca_n_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precision (Class 1): {:4.3f}%\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprec_1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mprec_n_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Recall (Class 1): {:4.3f}%\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreca_1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mreca_n_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# Get device (cpu vs gpu) specified:\n",
    "device = torch.device(params[\"device\"])\n",
    "if params[\"device\"][0:4] == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    enable_amp = True\n",
    "else:\n",
    "    enable_amp = False\n",
    "if enable_amp:\n",
    "    import apex.amp as amp\n",
    "\n",
    "# Get training info from param file:\n",
    "n_epochs = params[\"n_epochs\"] #100\n",
    "\n",
    "# Load in training dataset:\n",
    "X = np.load(params[\"training_dset_X\"])\n",
    "Y = np.load(params[\"training_dset_Y\"])\n",
    "print(\"Training dataset info:\")\n",
    "print(\"Shape of X:\", X.shape, \"Shape of Y\", Y.shape)\n",
    "dataset = phaselink_train.MyDataset(X, Y, device)\n",
    "\n",
    "# Get dataset info:\n",
    "n_samples = len(dataset)\n",
    "indices = list(range(n_samples))\n",
    "\n",
    "# Set size of training and validation subset:\n",
    "n_test = int(0.1*X.shape[0])\n",
    "validation_idx = np.random.choice(indices, size=n_test, replace=False)\n",
    "train_idx = list(set(indices) - set(validation_idx))\n",
    "\n",
    "# Specify samplers:\n",
    "train_sampler = phaselink_train.SubsetRandomSampler(train_idx)\n",
    "validation_sampler = phaselink_train.SubsetRandomSampler(validation_idx)\n",
    "\n",
    "# Load training data:\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    sampler=train_sampler\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    sampler=validation_sampler\n",
    ")\n",
    "\n",
    "stackedgru = phaselink_train.StackedGRU()\n",
    "stackedgru = stackedgru.to(device)\n",
    "#stackedgru = torch.nn.DataParallel(stackedgru,\n",
    "#    device_ids=['cuda:2', 'cuda:3', 'cuda:4', 'cuda:5'])\n",
    "\n",
    "if enable_amp:\n",
    "    #amp.register_float_function(torch, 'sigmoid')\n",
    "    from apex.optimizers import FusedAdam\n",
    "    optimizer = FusedAdam(stackedgru.parameters())\n",
    "    stackedgru, optimizer = amp.initialize(\n",
    "        stackedgru, optimizer, opt_level='O2')\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(stackedgru.parameters())\n",
    "\n",
    "model = phaselink_train.Model(stackedgru, optimizer, \\\n",
    "    model_path='./phaselink_model')\n",
    "print(\"Begin training process.\")\n",
    "model.train(train_loader, val_loader, n_epochs, enable_amp=enable_amp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "illegal-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For emptying memory on GPU:\n",
    "# torch.cuda.empty_cache()\n",
    "# del(model)\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download trained model, if using colab:\n",
    "if use_google_colab:\n",
    "    from google.colab import files\n",
    "    !zip -r ./phaselink_model/phaselink_model.zip ./phaselink_model\n",
    "    files.download('phaselink_model/phaselink_model.zip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "correct-concentration",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "version_ <= kMaxSupportedFileFormatVersion INTERNAL ASSERT FAILED at /Users/distiller/project/conda/conda-bld/pytorch_1579022061893/work/caffe2/serialize/inline_container.cc:132, please report a bug to PyTorch. Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 2. Your PyTorch installation may be too old. (init at /Users/distiller/project/conda/conda-bld/pytorch_1579022061893/work/caffe2/serialize/inline_container.cc:132)\nframe #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 135 (0x19d6b6267 in libc10.dylib)\nframe #1: caffe2::serialize::PyTorchStreamReader::init() + 2350 (0x19858650e in libtorch.dylib)\nframe #2: caffe2::serialize::PyTorchStreamReader::PyTorchStreamReader(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 143 (0x198585b5f in libtorch.dylib)\nframe #3: void pybind11::cpp_function::initialize<void pybind11::detail::initimpl::constructor<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >::execute<pybind11::class_<caffe2::serialize::PyTorchStreamReader>, 0>(pybind11::class_<caffe2::serialize::PyTorchStreamReader>&)::'lambda'(pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >), void, pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::detail::is_new_style_constructor>(pybind11::class_<caffe2::serialize::PyTorchStreamReader>&&, void (*)(pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::detail::is_new_style_constructor const&)::'lambda'(pybind11::detail::function_call&)::operator()(pybind11::detail::function_call&) const + 147 (0x196564413 in libtorch_python.dylib)\nframe #4: pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 3372 (0x195f5e1fc in libtorch_python.dylib)\n<omitting python frames>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-39f44db37962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mf_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_losses.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_fname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels_fnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_curr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mf_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_curr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/phaselink/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/phaselink/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: version_ <= kMaxSupportedFileFormatVersion INTERNAL ASSERT FAILED at /Users/distiller/project/conda/conda-bld/pytorch_1579022061893/work/caffe2/serialize/inline_container.cc:132, please report a bug to PyTorch. Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 2. Your PyTorch installation may be too old. (init at /Users/distiller/project/conda/conda-bld/pytorch_1579022061893/work/caffe2/serialize/inline_container.cc:132)\nframe #0: c10::Error::Error(c10::SourceLocation, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 135 (0x19d6b6267 in libc10.dylib)\nframe #1: caffe2::serialize::PyTorchStreamReader::init() + 2350 (0x19858650e in libtorch.dylib)\nframe #2: caffe2::serialize::PyTorchStreamReader::PyTorchStreamReader(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 143 (0x198585b5f in libtorch.dylib)\nframe #3: void pybind11::cpp_function::initialize<void pybind11::detail::initimpl::constructor<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >::execute<pybind11::class_<caffe2::serialize::PyTorchStreamReader>, 0>(pybind11::class_<caffe2::serialize::PyTorchStreamReader>&)::'lambda'(pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >), void, pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::detail::is_new_style_constructor>(pybind11::class_<caffe2::serialize::PyTorchStreamReader>&&, void (*)(pybind11::detail::value_and_holder&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >), pybind11::name const&, pybind11::is_method const&, pybind11::sibling const&, pybind11::detail::is_new_style_constructor const&)::'lambda'(pybind11::detail::function_call&)::operator()(pybind11::detail::function_call&) const + 147 (0x196564413 in libtorch_python.dylib)\nframe #4: pybind11::cpp_function::dispatcher(_object*, _object*, _object*) + 3372 (0x195f5e1fc in libtorch_python.dylib)\n<omitting python frames>\n"
     ]
    }
   ],
   "source": [
    "# Plot model training and validation loss to select best model:\n",
    "# (Note: This must currently be done on the same machine/machine architecture as \n",
    "#  the training was undertaken on).\n",
    "\n",
    "# Write the models loss function values to file:\n",
    "models_dir = \"phaselink_model\"\n",
    "models_fnames = list(glob.glob(os.path.join(models_dir, \"model_???_*.pt\")))\n",
    "models_fnames.sort()\n",
    "val_losses = []\n",
    "f_out = open(os.path.join(models_dir, 'val_losses.txt'), 'w')\n",
    "for model_fname in models_fnames:\n",
    "    model_curr = torch.load(model_fname)\n",
    "    val_losses.append(model_curr['loss'])\n",
    "    f_out.write(' '.join((model_fname, str(model_curr['loss']), '\\n')))\n",
    "    del(model_curr)\n",
    "    gc.collect()\n",
    "f_out.close()\n",
    "val_losses = np.array(val_losses)\n",
    "print(\"Written losses to file: \", os.path.join(models_dir, 'val_losses.txt'))\n",
    "\n",
    "# And select approximate best model (approx corner of loss curve):\n",
    "approx_corner_idx = np.argwhere(val_losses < np.average(val_losses))[0][0]\n",
    "print(\"Model to use:\", models_fnames[approx_corner_idx])\n",
    "\n",
    "# And plot:\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(val_losses)), val_losses)\n",
    "plt.hlines(val_losses[approx_corner_idx], 0, len(val_losses), color='r', ls=\"--\")\n",
    "plt.ylabel(\"Val loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()\n",
    "\n",
    "# And convert model to use to universally usable format (GPU or CPU):\n",
    "model = phaselink_train.StackedGRU().cuda(device)\n",
    "checkpoint = torch.load(models_fnames[approx_corner_idx], map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "torch.save(model, os.path.join(models_dir, 'model_to_use.gpu.pt'), _use_new_zipfile_serialization=False)\n",
    "new_device = \"cpu\"\n",
    "model = model.to(new_device)\n",
    "torch.save(model, os.path.join(models_dir, 'model_to_use.cpu.pt'), _use_new_zipfile_serialization=False)\n",
    "del model\n",
    "gc.collect()\n",
    "if use_google_colab:\n",
    "  files.download(os.path.join('phaselink_model','model_to_use.gpu.pt'))\n",
    "  files.download(os.path.join('phaselink_model','model_to_use.cpu.pt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-craps",
   "metadata": {},
   "source": [
    "## 3. Perform phase association on some real earthquakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hydraulic-punishment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eart0504/opt/anaconda3/envs/phaselink/lib/python3.7/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/Users/eart0504/opt/anaconda3/envs/phaselink/lib/python3.7/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.GRU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StackedGRU(\n",
       "  (fc1): Linear(in_features=5, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc3): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc4): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc5): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (fc6): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (gru1): GRU(32, 128, batch_first=True, bidirectional=True)\n",
       "  (gru2): GRU(256, 128, batch_first=True, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load correct model:\n",
    "if use_google_colab:\n",
    "    params[\"model_file\"] = \"phaselink_model/model_to_use.gpu.pt\"\n",
    "    model = torch.load(params[\"model_file\"])\n",
    "    \n",
    "else:\n",
    "    params[\"model_file\"] = \"phaselink_model/model_to_use.cpu.pt\"\n",
    "    model = torch.load(params[\"model_file\"])\n",
    "\n",
    "# And evaluate model:\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dressed-football",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading GPD file\n",
      "Finished reading GPD file, 100000 triggers found\n",
      "Day 001: 67693 gpd picks, 0 cumulative events detected\n",
      "Permuting sequence for all lags...\n",
      "Finished permuting sequence\n",
      "Predicting labels for all phases\n",
      "Finished label prediction\n",
      "Linking phases\n",
      "20 events detected initially\n",
      "Removing duplicates\n",
      "13 events detected after duplicate removal\n",
      "13 events left after applying n_min_det threshold\n",
      "Day 002: 32307 gpd picks, 13 cumulative events detected\n",
      "Permuting sequence for all lags...\n",
      "Finished permuting sequence\n",
      "Predicting labels for all phases\n",
      "Finished label prediction\n",
      "Linking phases\n",
      "36 events detected initially\n",
      "Removing duplicates\n",
      "35 events detected after duplicate removal\n",
      "35 events left after applying n_min_det threshold\n",
      "48 detections total\n",
      "Events output to .nlloc file.\n"
     ]
    }
   ],
   "source": [
    "# Detect events:\n",
    "X, labels = phaselink_eval.read_gpd_output(params)\n",
    "phaselink_eval.detect_events(X, labels, model, params)\n",
    "\n",
    "print(\"Events output to .nlloc file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-husband",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
